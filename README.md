CONVERSATIONAL RAG BACKEND - README

Project Overview:
This project implements a Conversational Retrieval-Augmented Generation (RAG) system using FastAPI. 
It provides two REST APIs: one for document ingestion and another for conversational querying.
The system allows users to upload documents (PDF or TXT), store embeddings in Qdrant, and 
retrieve relevant information using embeddings-based search. Multi-turn conversations are supported 
with chat memory stored in Redis. Additionally, users can book interviews via a dedicated API.

Features:
1. Document Ingestion API:
   - Upload PDF or TXT files.
   - Extract text from documents.
   - Chunk text using two strategies: fixed-size chunks or semantic sentence-based chunks.
   - Generate embeddings using SentenceTransformers (all-MiniLM-L6-v2).
   - Store embeddings in Qdrant vector database.
   - Store document metadata in SQL database.

2. Conversational RAG API:
   - Handle multi-turn conversational queries.
   - Retrieve relevant chunks from Qdrant based on query embeddings.
   - Maintain chat history per session using Redis.
   - Generate responses by summarizing top-k retrieved chunks.

3. Interview Booking API:
   - Users can book interviews by providing name, email, date, and time.
   - Booking info is stored in SQL database.
   - Returns confirmation with booking ID.

Tech Stack:
- FastAPI (backend framework)
- PyPDF2 (PDF text extraction)
- SentenceTransformers (embedding generation)
- Qdrant (vector database for embeddings)
- Redis (chat memory)
- SQLAlchemy + SQLite/PostgreSQL (metadata and bookings)
- Python 3.13

Workflow:
1. User uploads document via /upload-document/ endpoint.
2. Document text is extracted and chunked.
3. Chunks are converted to embeddings.
4. Embeddings are stored in Qdrant.
5. Metadata is stored in SQL DB.
6. User sends a query to /chat/ endpoint.
7. Query is converted to embedding.
8. Top-k relevant chunks are retrieved from Qdrant.
9. Response is generated by combining relevant chunks.
10. Chat history is updated in Redis.
11. Response is returned to the user.
12. Users can book interviews using /book/ endpoint, stored in SQL DB.

How to Run:
1. Clone the repository.
2. Create and activate a Python virtual environment.
3. Install dependencies: pip install -r requirements.txt
4. Start Qdrant (Docker recommended): docker run -p 6333:6333 -v <local_path>:/qdrant/storage qdrant/qdrant
5. Start Redis server locally or use Docker.
6. Run FastAPI app: uvicorn main:app --reload
7. Access API docs at http://localhost:8000/docs

Testing the APIs:
1. /upload-document/ - Upload a PDF or TXT, select chunking strategy.
2. /chat/ - POST a JSON payload with session_id and query:
   {
   
       "session_id": "test1234",
       "query": "What is the PDF about?"
   
   }
4. /book/ - POST booking info:
   {
   
       "name": "John Doe",
       "email": "john@example.com",
       "date": "2025-10-20",
       "time": "10:00"
   }

Future Improvements:
- Add advanced response generation using LLMs (Gemini or similar) for better summarization.
- Support additional document formats (Word, HTML, etc.).
- Implement user authentication and session management.
- Dockerize the entire application for production deployment.

Notes:
- All embeddings are stored in Qdrant and retrievable using cosine similarity.
- SQL DB stores metadata and bookings for easy tracking.
- Redis ensures chat memory for multi-turn conversations.
- The system currently summarizes retrieved chunks to generate concise responses.
- Only PDF and TXT document uploads are supported at the moment.

